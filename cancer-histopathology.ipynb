{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Summary**\n\nIn this project, I created a Convolutional Neural Network model to identify metastatic cancer in small image patches taken from larger digital pathology scans. The model achieved great scores for accuracy (88%), precision (88%), recall (88%), F1-score (88%) and Auc score (93%). "},{"metadata":{},"cell_type":"markdown","source":"**The dataset**\n\nThe data for this competition is a slightly modified version of the PatchCamelyon (PCam) benchmark dataset (the original PCam dataset contains duplicate images due to its probabilistic sampling, however, the version presented on Kaggle does not contain duplicates).\n[PCam] packs the clinically-relevant task of metastasis detection into a straight-forward binary image classification task, akin to CIFAR-10 and MNIST. Models can easily be trained on a single GPU in a couple hours, and achieve competitive scores in the Camelyon16 tasks of tumor detection and whole-slide image diagnosis. Furthermore, the balance between task-difficulty and tractability makes it a prime suspect for fundamental machine learning research on topics as active learning, model uncertainty, and explainability."},{"metadata":{},"cell_type":"markdown","source":"**Importing libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('../input/histopathologic-cancer-detectiont'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy.random import seed\nseed(101)\n\nimport pandas as pd\nimport numpy as np\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\n\nimport os\nimport cv2\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntf.random.set_seed(101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Setting prerequisites**"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_size=96\nimg_ch=3\nsample_size=50000  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exploring the directory**"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/histopathologic-cancer-detection')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data exploration**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(os.listdir('../input/histopathologic-cancer-detection/train')))\nprint(len(os.listdir('../input/histopathologic-cancer-detection/test')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data = pd.read_csv('../input/histopathologic-cancer-detection/train_labels.csv')\n\n# removing this image because it caused a training error previously\ndf_data[df_data['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n\n# removing this image because it's black\ndf_data[df_data['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']\n\nprint(df_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_count = df_data.label.value_counts()\n\n%matplotlib inline\nplt.pie(labels_count, labels=['No Cancer', 'Cancer'], startangle=180, \n        autopct='%1.1f', colors=['green','red'], shadow=True)\nplt.figure(figsize=(15,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create the Train and Validation Sets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_0=df_data[df_data['label']==0].sample(sample_size,random_state=101)\ndf_1=df_data[df_data['label']==1].sample(sample_size,random_state=101)\n\n# concat the dataframes\ndf_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n# shuffle\ndf_data = shuffle(df_data)\n\ndf_data['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data splitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_data['label']\n\ndf_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n\nprint(df_train.shape)\nprint(df_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating a new directory for the ImageDataGenerator**\n\nLet's create 2 folders inside 'base_dir':\n1.train_dir: it contains:\n    # healthy_tissue\n    # tumor_tissue\n\n2.val_dir: it contains:\n    # healthy_tissue\n    # tumor_tissue"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_dir='base_dir'\nos.mkdir(base_dir)\n\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\ntrain_healthy_tissue_dir = os.path.join(train_dir, 'healthy_tissue')\nos.mkdir(train_healthy_tissue_dir)\ntrain_tumor_tissue_dir = os.path.join(train_dir, 'tumor_tissue')\nos.mkdir(train_tumor_tissue_dir)\n\n\n# create new folders inside val_dir\ntest_healthy_tissue_dir = os.path.join(val_dir, 'healthy_tissue')\nos.mkdir(test_healthy_tissue_dir)\ntest_tumor_tissue_dir = os.path.join(val_dir, 'tumor_tissue')\nos.mkdir(test_tumor_tissue_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check that the folders have been created\nos.listdir('base_dir/train_dir')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the id as the index in df_data\ndf_data.set_index('id', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a list of train and val images\ntrain_list = list(df_train['id'])\nval_list = list(df_val['id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transfer the train images\n\nfor image in train_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image + '.tif'\n    # get the label for a certain image\n    target = df_data.loc[image,'label']\n    \n    # these must match the folder names\n    if target == 0:\n        label = 'healthy_tissue'\n    if target == 1:\n        label = 'tumor_tissue'\n    \n    # source path to image\n    src = os.path.join('../input/histopathologic-cancer-detection/train', fname)\n    # destination path to image\n    dst = os.path.join(train_dir, label, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n\n\n# Transfer the val images\n\nfor image in val_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image + '.tif'\n    # get the label for a certain image\n    target = df_data.loc[image,'label']\n    \n    # these must match the folder names\n    if target == 0:\n        label = 'healthy_tissue'\n    if target == 1:\n        label = 'tumor_tissue'\n    \n\n    # source path to image\n    src = os.path.join('../input/histopathologic-cancer-detection/train', fname)\n    # destination path to image\n    dst = os.path.join(val_dir, label, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir/train_dir/healthy_tissue')))\nprint(len(os.listdir('base_dir/train_dir/tumor_tissue')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir/val_dir/healthy_tissue')))\nprint(len(os.listdir('base_dir/val_dir/tumor_tissue')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#training\ntrain_tumor_names = os.listdir(train_tumor_tissue_dir)\nprint(train_tumor_names[:5])\ntrain_healthy_names = os.listdir(train_healthy_tissue_dir)\nprint(train_healthy_names[:5])\n#test\ntest_tumor_names = os.listdir(test_tumor_tissue_dir)\nprint(test_tumor_names[:5])\ntest_healthy_names = os.listdir(test_healthy_tissue_dir)\nprint(test_healthy_names[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the filenames from the healthy and tumor dir of the train dataset\nhealthy_pic =[os.path.join(train_healthy_tissue_dir, filename) for filename in train_healthy_names[0:4]]\ntumor_pic =[os.path.join(train_tumor_tissue_dir, filename) for filename in train_tumor_names[0:4]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n# plot a grid of 8 images (4 images of each class)\nimport matplotlib.image as mpimg\n#set the number of columns and rows\nrows = 4\ncols = 2\n#set the figure size\nfig = plt.gcf()\nfig.set_size_inches(15,15)\n#merge the covid and normal list\nmerged_list = healthy_pic+tumor_pic\nfor i , img_path in enumerate(merged_list):\n  data= img_path.split('/', 2)[2] \n  sp = plt.subplot(rows, cols, i + 1)\n  sp.axis('Off')\n  img = mpimg.imread(img_path)\n  sp.set_title(data, fontsize=11)\n  plt.imshow(img,cmap='gray')\n\n  plt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Augmentation**\n\nThe amount of data gathered was very low and could cause the models to under-fit. Hence, we would use a brilliant technique of Data Augmentation to increase the amount of data. This technique relies on rotations, flips, change in exposure, etc to create similar images. Using this technique we can increase the size of data by a high factor. The output image of the cropping stage is given as input to ImageDataGenerator which is a function in keras.preprocessing.image library. This function takes multiple arguments that decide how Augmentation takes place."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the generators\ntrain_path = 'base_dir/train_dir'\nvalid_path = 'base_dir/val_dir'\ntest_path = '../input/histopathologic-cancer-detection/test'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\n\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)\ndatagen = ImageDataGenerator(rescale=1.0/255)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(img_size,img_size),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(img_size,img_size),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical')\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(img_size,img_size),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nsample_x,sample_y = next(train_gen)\nplt.figure(figsize=(12,9))\nfor i in range(6):\n    plt.subplot(2,3,i+1)\n    sample = image.array_to_img(sample_x[i])\n    plt.xlabel(np.argmax(sample_y[i]))\n    plt.imshow(sample)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the class indices\ntrain_gen.class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the image shape\ntrain_gen.image_shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Building & Training**\n\nSo until now we have cleaned, pre-processed, augmented, pickled the data and now we are ready to feed it to a ‘Neural Network’ and see how it performs. We will try to perform image classification task by using a CNN model."},{"metadata":{},"cell_type":"markdown","source":"**CNN model**\n\nI used a great CNN architecture which consisted of four convolutional layers (256, 128, 128, and 64 filters), one flatten layer and four fully connected layers (128, 64, 8 and 2 neurons). To achieve a better understanding of the model, I considered as metric not only accuracy, but I added Precision, Recall, Auc score and F1 score for each epoch."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.python.keras import Sequential\nfrom tensorflow.keras import layers, optimizers\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\nimport tensorflow.keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n# add the convolutional layer\n# filters, size of filters,padding,activation_function,input_shape\nmodel.add(Conv2D(256,(3,3), padding='SAME', activation='relu', input_shape=(96,96,3)))\n# pooling layer\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n# place a dropout layer\nmodel.add(Dropout(0.2))\n# add the second convolutional layer\nmodel.add(Conv2D(128,(3,3), padding='SAME', activation='relu'))\n# pooling layer\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n# place a dropout layer\nmodel.add(Dropout(0.2))\n# add the third convolutional layer\nmodel.add(Conv2D(128,(2,2), padding='SAME', activation='relu'))\n# pooling layer\nmodel.add(MaxPooling2D(pool_size=(1,1)))\n# place a dropout layer\nmodel.add(Dropout(0.2))\n# add the fourth convolutional layer\nmodel.add(Conv2D(64,(1,1), padding='SAME', activation='relu'))\n# pooling layer\nmodel.add(MaxPooling2D(pool_size=(1,1)))\n# place a dropout layer\nmodel.add(Dropout(0.2))\n# Flatten layer\nmodel.add(Flatten())\n# add three dense layers : amount of nodes, activation\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(8,activation='relu'))\n# place a dropout layer\n# 0.5 drop out rate is recommended, half input nodes will be dropped at each update\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2,activation='softmax'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom tensorflow.keras.utils import plot_model\nplot_model(model, 'CNN-model.png', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Compile & Train the CNN Model**\n\nTo amplify our understanding of the model and its performance, we assess different metrics: loss function, accuracy, precision, recall anmd F1 scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"def f1_score(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's define the metrics**"},{"metadata":{"trusted":true},"cell_type":"code","source":"METRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),  \n      tf.keras.metrics.AUC(name='auc'),\n        f1_score,\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1 **(epoch / s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(0.01, 5) # when i run it for 50 epochs\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(Adam(lr=0.0001),loss='binary_crossentropy',metrics=METRICS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Training the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = \"CNN-model\"\n\n#callbacks\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=50, verbose=1,\n                    callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Performance Evaluation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the keys of history object\n\nhistory.history.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Train_Val_Plot(acc,val_acc,loss,val_loss,auc,val_auc,precision,val_precision,f1,val_f1):\n  fig, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1,5, figsize= (20,5))\n  fig.suptitle(\" MODEL METRICS VISUALIZATION \")\n  ax1.plot(range(1, len(acc) + 1), acc)\n  ax1.plot(range(1, len(val_acc) + 1), val_acc)\n  ax1.set_title('History of Accuracy')\n  ax1.set_xlabel('Epochs')\n  ax1.set_ylabel('Accuracy')\n  ax1.legend(['training', 'validation'])\n  ax2.plot(range(1, len(loss) + 1), loss)\n  ax2.plot(range(1, len(val_loss) + 1), val_loss)\n  ax2.set_title('History of Loss')\n  ax2.set_xlabel('Epochs')\n  ax2.set_ylabel('Loss')\n  ax2.legend(['training', 'validation'])\n    \n  ax3.plot(range(1, len(auc) + 1), auc)\n  ax3.plot(range(1, len(val_auc) + 1), val_auc)\n  ax3.set_title('History of AUC')\n  ax3.set_xlabel('Epochs')\n  ax3.set_ylabel('AUC')\n  ax3.legend(['training', 'validation'])\n\n  ax4.plot(range(1, len(precision) + 1), precision)\n  ax4.plot(range(1, len(val_precision) + 1), val_precision)\n  ax4.set_title('History of Precision')\n  ax4.set_xlabel('Epochs')\n  ax4.set_ylabel('Precision')\n  ax4.legend(['training', 'validation'])\n    \n  ax5.plot(range(1, len(f1) + 1), f1)\n  ax5.plot(range(1, len(val_f1) + 1), val_f1)\n  ax5.set_title('History of F1-score')\n  ax5.set_xlabel('Epochs')\n  ax5.set_ylabel('F1 score')\n  ax5.legend(['training', 'validation'])\n\n\n  plt.show()\n\nTrain_Val_Plot(history.history['accuracy'],history.history['val_accuracy'],\n                 history.history['loss'],history.history['val_loss'],\n                 history.history['auc'],history.history['val_auc'],\n                 history.history['precision'],history.history['val_precision'],\n                 history.history['f1_score'],history.history['val_f1_score']\n                 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the labels that are associated with each index\nprint(val_gen.class_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model.evaluate(test_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"CNN Accuracy = \", scores[1])\nprint(\"CNN Precision = \", scores[2])\nprint(\"CNN Recall = \", scores[3])\nprint(\"CNN AUC = \", scores[4])\nprint(\"CNN F1_score = \", scores[5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, we achieved a good overall performance for all the metrics! So we can conclude that our CNN model could be a good piece of work in histopathology for classification tasks."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}